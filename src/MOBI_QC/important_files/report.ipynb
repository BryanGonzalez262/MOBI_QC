{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reportlab.lib.styles import ParagraphStyle, getSampleStyleSheet\n",
    "from reportlab.lib.pagesizes import A4\n",
    "from reportlab.lib import colors\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, Image\n",
    "from reportlab.lib.utils import ImageReader\n",
    "import os\n",
    "import mne\n",
    "import json\n",
    "\n",
    "from utils import *\n",
    "from eeg import compute_eeg_pipeline, test_eeg_pipeline\n",
    "from ecg_qc import ecg_qc \n",
    "from eda_qc import eda_qc\n",
    "from rsp_qc import *\n",
    "from mic_qc import *\n",
    "from lsl_problem import *\n",
    "from et_qc import *\n",
    "from webcam_qc import *\n",
    "import matplotlib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = \"P5415639\"\n",
    "\n",
    "xdf_filename = f'/Users/bryan.gonzalez/CUNY_subs/sub-{subject}/sub-{subject}_ses-S001_task-CUNY_run-001_mobi.xdf'\n",
    "\n",
    "#xdf_filename = f'/Users/camilla.strauss/Desktop/CUNY_Data/Data/sub-{subject}/sub-{subject}_ses-S001_task-CUNY_run-001_mobi.xdf'\n",
    "\n",
    "#xdf_filename = #Apurva's xdf file\n",
    "\n",
    "video_filename = '/'.join(xdf_filename.split('/')[:-1])+ f'/sub-{subject}_ses-S001_task-CUNY_run-001_video.avi'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_df = import_stim_data(xdf_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. EEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the eeg vars\n",
    "eeg_vars, raw_cleaned, ica, eeg_df = compute_eeg_pipeline(xdf_filename, \n",
    "                                                          stim_df=stim_df, \n",
    "                                                          task='RestingState')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_print = {\n",
    "    'Bad channels before robust reference': eeg_vars['bad_channels_before'], \n",
    "    'Interpolated channels': eeg_vars['interpolated_channels'], \n",
    "    'Bad channels after interpolation': eeg_vars['bad_channels_after'], \n",
    "    'Percent Good?': f\"{eeg_vars['percent_good']:.4}%\" \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Artifact Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ica.plot_components( title='ICA Components')\n",
    "# Save the ICA plot\n",
    "\n",
    "fig.savefig(f'../report_images/sub-{subject}_ica_components.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica.plot_properties(raw_cleaned, picks=[0,1,2,3,4]) # This exact component number probably won't work if you recompute ICA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica.plot_overlay(raw_cleaned, exclude=[0,1, 2,3]) # see what the data would look like if we removed the component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica.exclude = [0,1,2,3,4] # these are the components that we want to exclude\n",
    "ica.apply(raw_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_vars['components_excluded'] = ica.exclude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_cleaned.annotations.delete([i for i, desc in enumerate(raw_cleaned.annotations.description) if desc == 'blink' or desc == 'BAD_muscle'])\n",
    "fig = raw_cleaned.plot(show_scrollbars=False,\n",
    "                        show_scalebars=False,events=None, start=0, \n",
    "                        duration=200,n_channels=50, scalings=.35e-4, color='k', title='EEG Data after ICA')\n",
    "\n",
    "fig.savefig(f'../report_images/sub-{subject}_cleaned_eeg.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "fig = raw_cleaned.plot_psd(fmax=50, average=False, show=True)\n",
    "fig.savefig(f'../report_images/sub-{subject}_cleaned_eeg_psd.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_cleaned_pathname = '/'.join(xdf_filename.split('/')[:-1]) + f'/sub-{subject}_ses-S001_task-CUNY_run-001_eeg_clean.fif'\n",
    "\n",
    "raw_cleaned.save(raw_cleaned_pathname, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ECG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ecg_vars, ecg_plt, ps_df] = ecg_qc(xdf_filename = xdf_filename, stim_df = stim_df, task= 'RestingState')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_vars.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_print = {\n",
    "    \"Effective sampling rate\": f'{ecg_vars[\"sampling_rate\"]:.4f} Hz', \n",
    "    \"Signal to Noise Ratio\": f'{ecg_vars[\"SNR\"]:.4f} dB',\n",
    "    \"Average heart rate\": f'{ecg_vars[\"average_heart_rate\"]:.4f} bpm',\n",
    "    \"Kurtosis signal quality index (kSQI)\": f'{ecg_vars[\"kurtosis_SQI\"]:.4f}',\n",
    "    \"Power spectrum distribution (pSQI)\": f'{ecg_vars[\"power_spectrum_distribution_SQI\"]:.4f} mVÂ²/Hz',\n",
    "    \"Relative power in baseline (basSQI)\": f'{ecg_vars[\"relative_baseline_power_sqi\"]:.4f}%'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[eda_vars, eda_plt1, eda_plt2, ps_df] = eda_qc(xdf_filename = xdf_filename, stim_df = stim_df, task= 'RestingState')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_vars.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_print = {\n",
    "    \"Effective sampling rate\": f'{eda_vars[\"sampling_rate\"]:.4f} Hz', \n",
    "    \"Signal to noise ratio\": f'{eda_vars[\"snr\"]:.4f} dB',\n",
    "    \"Signal integrity check\": f'{eda_vars[\"signal_integrity_check\"]:.4f}%',\n",
    "    \"Average skin conductance level\": f'{eda_vars[\"average_scl\"]:.4f} mS',\n",
    "    \"Skin conductance level std\": f'{eda_vars[\"scl_sd\"]:.4f} mS',\n",
    "    \"Skin conductance level coefficient of variation\": f'{eda_vars[\"scl_cv\"]:.4f}%',\n",
    "    \"Average amplitude of skin conductance response\": f'{eda_vars[\"average_scr_amplitude\"]:.4f} mS',\n",
    "    \"Skin conductance response validity\": f'{eda_vars[\"sc_validity\"]:.4f} %' # this will need to be changed to [scr_validity]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsp_vars, ps_df = rsp_qc(xdf_filename = xdf_filename, stim_df = stim_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsp_print = {\n",
    "    \"Effective sampling rate\": f'{rsp_vars[\"sampling_rate\"]:.4f} Hz', \n",
    "    \"Signal to noise ratio\": f'{rsp_vars[\"rsp_snr\"]:.4f} dB',\n",
    "    \"Breath amplitude mean\": f'{rsp_vars[\"breath_amplitude_mean\"]:.4f} V',\n",
    "    \"Breath amplitude std\": f'{rsp_vars[\"breath_amplitude_std\"]:.4f} V',\n",
    "    \"Breath amplitude range\": f'{rsp_vars[\"breath_amplitude_range\"]} V',\n",
    "    \"Respiration rate mean\": f'{rsp_vars[\"rsp_rate_mean\"]:.4f} bpm',\n",
    "    \"Respiration rate std\": f'{rsp_vars[\"rsp_rate_std\"]:.4f} bpm', \n",
    "    \"Respiration rate range\": f'{rsp_vars[\"rsp_rate_range\"]} bpm', \n",
    "    \"Peak to peak interval mean\": f'{rsp_vars[\"ptp_mean\"]:.4f} sec', \n",
    "    \"Peak to peak interval std\": f'{rsp_vars[\"ptp_std\"]:.4f} sec', \n",
    "    \"Peak to peak interval range\": f'{rsp_vars[\"ptp_range\"]} sec', \n",
    "    \"Baseline drift\": f'{rsp_vars[\"baseline_drift\"]:.4f} V', \n",
    "    \"Autocorrelation at typical breath cycle\": f'{rsp_vars[\"autocorrelation\"]:.4f}'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsp_print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Mic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mic_vars, mic_df = mic_qc(xdf_filename = xdf_filename, stim_df = stim_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mic_print = {\n",
    "    \"Effective sampling rate\": f'{mic_vars[\"sampling_rate\"]:.4f} Hz', \n",
    "    \"Difference between .wav file and lsl timestamps durations\": f'{mic_vars[\"lsl_wav_duration_diff\"]:.4f} sec', \n",
    "    \"Number of NaN's\": f'{mic_vars[\"num_NaN\"]}',\n",
    "    \"Percent of NaN's\": f'{mic_vars[\"percent_NaN\"]:.4f}%',\n",
    "    \"Mic samples first quartile\": f'{mic_vars[\"quan25\"]:.4f}',\n",
    "    \"Mic samples third quartile\": f'{mic_vars[\"quan75\"]:.4f}',\n",
    "    \"Mic samples std\": f'{mic_vars[\"std\"]:.4f}',\n",
    "    \"Mic samples min\": f'{mic_vars[\"min\"]:.4f}',\n",
    "    \"Mic samples max\": f'{mic_vars[\"max\"]:.4f}'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RestingState duration matches camera duration!\n",
      "RestingState:  300.00033711252036\n",
      "Webcam Stream:  299.96376050871913\n",
      "Error: Could not open video file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: Couldn't read video stream from file \"/Users/bryan.gonzalez/CUNY_subs/sub-P5415639/sub-P5415639_ses-S001_task-CUNY_run-001_video.avi\"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m webcam_vars, cam_df = \u001b[43mwebcam_qc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxdf_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxdf_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m                                \u001b[49m\u001b[43mstim_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstim_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                                \u001b[49m\u001b[43mvideo_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvideo_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mRestingState\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MOBI_QC/src/MOBI_QC/important_files/webcam_qc.py:174\u001b[39m, in \u001b[36mwebcam_qc\u001b[39m\u001b[34m(xdf_filename, video_file, stim_df, task)\u001b[39m\n\u001b[32m    171\u001b[39m \u001b[38;5;28mvars\u001b[39m[\u001b[33m'\u001b[39m\u001b[33msampling_rate\u001b[39m\u001b[33m'\u001b[39m] = sampling_rate\n\u001b[32m    172\u001b[39m cam_df[\u001b[33m'\u001b[39m\u001b[33mface_detected\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m fc, face_frames, frames_checked = count_faces_in_video(video_file, cam_df=cam_df, stim_df=stim_df, task=task, frame_skip=\u001b[32m10\u001b[39m)\n\u001b[32m    175\u001b[39m frames_without_faces = [frame \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m frames_checked \u001b[38;5;28;01mif\u001b[39;00m frame \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m face_frames]\n\u001b[32m    176\u001b[39m face_perc = fc/\u001b[38;5;28mlen\u001b[39m(frames_checked)\n",
      "\u001b[31mValueError\u001b[39m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "webcam_vars, cam_df = webcam_qc(xdf_filename=xdf_filename,\n",
    "                                stim_df=stim_df,\n",
    "                                video_file=video_filename, task='RestingState')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[fc, face_frames, frames_checked] = count_faces_in_video(video_path=video_filename,\n",
    "                                                       task=\"RestingState\", \n",
    "                                                       cam_df=get_event_data(event=\"RestingState\", \n",
    "                                                                             df=import_video_data(xdf_filename),\n",
    "                                                                             stim_df=stim_df),\n",
    "                                                       stim_df=stim_df,\n",
    "                                                       frame_skip=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webcam_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webcam_print = {\n",
    "    \"Effective sampling rate\": f'{webcam_vars[\"sampling_rate\"]:.4f} Hz', \n",
    "    \"Collected full resting state\": webcam_vars[\"collected_full_RestingState\"], \n",
    "    \"Percent of frames with face detected\": f'{webcam_vars[\"face_perc\"]:.4%}'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webcam_print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_vars, et_df = et_qc(xdf_filename = xdf_filename, stim_df = stim_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_print = {\n",
    "    \"Effective sampling rate\": f'{et_vars[\"sampling_rate\"]:.4f} Hz', \n",
    "    \"Flag: all coordinates have the same % validity within each measure (LR, gaze point/origin/diameter)\": et_vars[\"flag1\"], \n",
    "    \"Flag: % of NaNs is the same between coordinate systems (UCS and TBCS (gaze origin) and between UCS and display area (gaze point))\": et_vars[\"flag2\"],\n",
    "    \"Mean difference in percent valid data between right and left eyes\": f'{et_vars[\"LR_mean_diff\"]:.4f}%',\n",
    "    \"Percent of data with gaze point differences of over 0.2 mm\": f'{et_vars[\"percent_over02\"]:.4f}%'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Durations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions are in utils.py (but should be called in the report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map = {\n",
    "    'et': et_df,\n",
    "    'ps': ps_df,\n",
    "    'mic': mic_df,\n",
    "    'cam': cam_df\n",
    "    }\n",
    "    # 'eeg': eeg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_vars = {\"Durations of each modality + comparison to expected duration:\": \n",
    "    get_durations(xdf_path=xdf_filename, task='Experiment', df_map = df_map, stim_df = stim_df)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_print = duration_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i wont run these but they are here for reference\n",
    "# get_durations('RestingState', xdf_filename)\n",
    "# get_durations('CampFriend', xdf_filename)\n",
    "# get_durations('SocialTask', xdf_filename)\n",
    "# whole_durations(xdf_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSL Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsl_vars = lsl_problem_qc(xdf_filename, df_map = df_map, stim_df = stim_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsl_print = {\n",
    "    \"Percent of missing data in entire experiment\": lsl_vars[\"percent_loss\"],\n",
    "    \"Percent of missing data before social task offset\": lsl_vars[\"loss_before_social_task\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I try from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modalities and corresponding data\n",
    "metric_names = [\"ECG\", \"EDA\", \"RSP\", \"MIC\", \"ET\", \"WEBCAM\", \"Stream Durations\", \"LSL\"]\n",
    "metrics_list = [ecg_vars, eda_vars, rsp_vars, mic_vars, et_vars, webcam_vars, duration_vars, lsl_vars]\n",
    "\n",
    "# PDF structure\n",
    "pdf_path = \"output_report.pdf\"\n",
    "doc = SimpleDocTemplate(pdf_path, pagesize=A4)\n",
    "elements = []\n",
    "styles = getSampleStyleSheet()\n",
    "\n",
    "# Define subtitle style if not already done\n",
    "subtitle_style = ParagraphStyle(\n",
    "    name=\"Subtitle\",\n",
    "    parent=styles[\"Heading2\"],\n",
    "    fontSize=14,\n",
    "    leading=16,\n",
    "    textColor=\"gray\",\n",
    "    spaceAfter=12,\n",
    "    alignment=1  # Centered\n",
    ")\n",
    "\n",
    "\n",
    "elements.append(Paragraph(f\"Subject Report: {subject}\", styles[\"Title\"]))\n",
    "elements.append(Paragraph(f\"Collection Date: {get_collection_date(xdf_filename)}\", subtitle_style))\n",
    "elements.append(Spacer(1, 12))\n",
    "\n",
    "# Format each metrics dict\n",
    "for name, metrics in zip(metric_names, metrics_list):\n",
    "    elements.append(Paragraph(name, styles[\"Heading2\"]))\n",
    "    for k, v in metrics.items():\n",
    "        if isinstance(v, pd.DataFrame):\n",
    "            data = [v.columns.tolist()] + v.values.tolist()  # Include headers\n",
    "            table = Table(data, repeatRows=1)\n",
    "            table.hAlign = 'LEFT'\n",
    "\n",
    "            table.setStyle(TableStyle([\n",
    "                ('BACKGROUND', (0, 0), (-1, 0), colors.lightgrey),\n",
    "                ('GRID', (0, 0), (-1, -1), 0.5, colors.black),\n",
    "                ('FONTNAME', (0, 0), (-1, -1), 'Helvetica'),\n",
    "                ('FONTSIZE', (0, 0), (-1, -1), 10),\n",
    "                ('ALIGN', (0, 0), (-1, -1), 'LEFT'),\n",
    "            ]))\n",
    "            elements.append(Paragraph(k, styles['Normal']))\n",
    "            elements.append(Spacer(1, 12))\n",
    "\n",
    "            elements.append(table)\n",
    "            elements.append(Spacer(1, 12))\n",
    "        else:\n",
    "            text = f\"<b>{k}:</b> {v:.4f}\" if isinstance(v, float) else f\"<b>{k}:</b> {v}\"\n",
    "            elements.append(Paragraph(text, styles[\"Normal\"]))\n",
    "    elements.append(Spacer(1, 12))\n",
    "\n",
    "    # images\n",
    "    folder = \"report_images\"\n",
    "    image_keyword = name.lower()\n",
    "    if os.path.exists(folder):\n",
    "        for fname in sorted(os.listdir(folder)):\n",
    "            if image_keyword in fname.lower() and subject in fname:\n",
    "                image_path = os.path.join(folder, fname)\n",
    "                img = Image(image_path, width=400, height=200)  # Adjust size as needed\n",
    "                elements.append(img)\n",
    "\n",
    "\n",
    "doc.build(elements)\n",
    "print(f'PDF created: {pdf_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "FileLink(\"output_report.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Report with formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modalities and corresponding data\n",
    "metric_names = [\"ECG\", \"EDA\", \"RSP\", \"MIC\",\"ET\", \"WEBCAM\", \"Stream Durations\", \"LSL\"]\n",
    "metrics_list = [ecg_print, eda_print, rsp_print, mic_print, et_print, webcam_print, duration_print, lsl_print]\n",
    "# metric_names = [\"ECG\", \"EDA\",\"WEBCAM\"]\n",
    "# metrics_list = [ecg_print, eda_print,  webcam_print]\n",
    "\n",
    "# PDF structure\n",
    "parent_folder = xdf_filename.split('mobi')[0]\n",
    "pdf_path = f\"{parent_folder}QCReport.pdf\"\n",
    "doc = SimpleDocTemplate(pdf_path, pagesize=A4)\n",
    "elements = []\n",
    "styles = getSampleStyleSheet()\n",
    "\n",
    "# Define subtitle style if not already done\n",
    "subtitle_style = ParagraphStyle(\n",
    "    name=\"Subtitle\",\n",
    "    parent=styles[\"Heading2\"],\n",
    "    fontSize=14,\n",
    "    leading=16,\n",
    "    textColor=\"gray\",\n",
    "    spaceAfter=12,\n",
    "    alignment=1  # Centered\n",
    ")\n",
    "\n",
    "# page number function\n",
    "def add_page_number(canvas, doc):\n",
    "    page_num = f'{canvas.getPageNumber()}'\n",
    "    canvas.setFont(\"Helvetica\", 9)\n",
    "    canvas.drawRightString(570, 20, page_num)  # (x, y) from bottom-left\n",
    "\n",
    "\n",
    "elements.append(Paragraph(f\"Subject Report: {subject}\", styles[\"Title\"]))\n",
    "elements.append(Paragraph(f\"Collection Date: {get_collection_date(xdf_filename)}\", subtitle_style))\n",
    "elements.append(Spacer(1, 12))\n",
    "\n",
    "# Format each metrics dict\n",
    "for name, metrics in zip(metric_names, metrics_list):\n",
    "    elements.append(Paragraph(name, styles[\"Heading2\"]))\n",
    "    for k, v in metrics.items():\n",
    "        if isinstance(v, pd.DataFrame):\n",
    "            data = [v.columns.tolist()] + v.values.tolist()  # Include headers\n",
    "            table = Table(data, repeatRows=1)\n",
    "            table.hAlign = 'LEFT'\n",
    "\n",
    "            table.setStyle(TableStyle([\n",
    "                ('BACKGROUND', (0, 0), (-1, 0), colors.lightgrey),\n",
    "                ('GRID', (0, 0), (-1, -1), 0.5, colors.black),\n",
    "                ('FONTNAME', (0, 0), (-1, -1), 'Helvetica'),\n",
    "                ('FONTSIZE', (0, 0), (-1, -1), 10),\n",
    "                ('ALIGN', (0, 0), (-1, -1), 'LEFT'),\n",
    "            ]))\n",
    "            elements.append(Paragraph(k, styles['Normal']))\n",
    "            elements.append(Spacer(1, 12))\n",
    "\n",
    "            elements.append(table)\n",
    "            elements.append(Spacer(1, 12))\n",
    "        else:\n",
    "            text = f\"<b>{k}:</b> {v}\" if isinstance(v, float) else f\"<b>{k}:</b> {v}\"\n",
    "            elements.append(Paragraph(text, styles[\"Normal\"]))\n",
    "    elements.append(Spacer(1, 12))\n",
    "\n",
    "    # images\n",
    "    folder = \"report_images\"\n",
    "    image_keyword = name.lower()\n",
    "    if os.path.exists(folder):\n",
    "        for fname in sorted(os.listdir(folder)):\n",
    "            if image_keyword in fname.lower() and subject in fname:\n",
    "                image_path = os.path.join(folder, fname)\n",
    "\n",
    "                # get dimensions\n",
    "                image_reader = ImageReader(image_path)\n",
    "                orig_width, orig_height = image_reader.getSize()\n",
    "                print(orig_width, orig_height)\n",
    "                if orig_width > 3000:\n",
    "                    img = Image(image_path, width=orig_width/9, height=orig_height/7)\n",
    "                elif orig_width > 1400:\n",
    "                    img = Image(image_path, width=orig_width/3, height=orig_height/3)\n",
    "                else:\n",
    "                    img = Image(image_path, width=orig_width/2, height=orig_height/2)  # Adjust size as needed\n",
    "                elements.append(img)\n",
    "                elements.append(Spacer(1, 12))\n",
    "\n",
    "\n",
    "\n",
    "doc.build(elements, onFirstPage = add_page_number, onLaterPages = add_page_number)\n",
    "print(f'PDF created: {pdf_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "FileLink(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = [\"ECG\", \"EDA\",\"WEBCAM\"]\n",
    "metrics_list = [ecg_print, eda_print,  webcam_print]\n",
    "\n",
    "for name, metrics in zip(metric_names, metrics_list):\n",
    "    print(name)\n",
    "    image_keyword = name.lower()\n",
    "\n",
    "    if os.path.exists(folder):\n",
    "        for fname in sorted(os.listdir(folder)):\n",
    "            if image_keyword in fname.lower() and subject in fname:\n",
    "                image_path = os.path.join(folder, fname)\n",
    "\n",
    "                # get dimensions\n",
    "                image_reader = ImageReader(image_path)\n",
    "                orig_width, orig_height = image_reader.getSize()\n",
    "                print(fname, orig_width, orig_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set up the document\n",
    "doc = SimpleDocTemplate(\"example_report.pdf\", pagesize=LETTER)\n",
    "styles = getSampleStyleSheet()\n",
    "story = []\n",
    "\n",
    "# Add a title\n",
    "title = Paragraph(f\"Subject Report: {subject}\", styles[\"Title\"])\n",
    "story.append(title)\n",
    "story.append(Spacer(1, 20))\n",
    "\n",
    "# Add a paragraph\n",
    "text = f\"\"\"\n",
    "Collection Date: {get_collection_date(xdf_filename)} \n",
    "\"\"\"\n",
    "paragraph = Paragraph(text, styles[\"BodyText\"])\n",
    "story.append(paragraph)\n",
    "story.append(Spacer(1, 20))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a subtitle\n",
    "subtitle = Paragraph(f\"EEG\", styles[\"Heading2\"], )\n",
    "story.append(subtitle)\n",
    "story.append(Spacer(1, 5))\n",
    "\n",
    "# Add a paragraph\n",
    "text = f\"\"\"\n",
    "Data preprocessed by performing <b>line noise removal</b>, <b>robust referencing</b>, and <b>bad channel detection/interpolation</b> using PyPrep pipeline. First, the pipeline applies a notch filter at 60 Hz and its harmonics to remove power line noise. Then, it performs <b>robust average referencing</b>, where it detects bad channels, interpolates them using surrounding signals, and computes a median-based reference across EEG channels. This ensures a stable reference even in the presence of noisy electrodes. The final output is a cleaned EEG dataset with a consistent reference, ready for further analysis.\n",
    "\"\"\"\n",
    "\n",
    "paragraph = Paragraph(text, styles[\"BodyText\"], )\n",
    "story.append(paragraph)\n",
    "story.append(Spacer(1, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer the EEG variables (will take time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add an image\n",
    "from reportlab.lib.units import inch\n",
    "from reportlab.platypus import Image\n",
    "image = Image(\"/Users/camilla.strauss/Desktop/MOBI_QC/src/MOBI_QC/report_images/P5287460_rsp_peaktopeak.png\", 7*inch, 3*inch)\n",
    "image.hAlign = 'CENTER'\n",
    "story.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story.append(Spacer(1, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Define the table style to make all borders white\n",
    "style = TableStyle([\n",
    "    ('GRID', (0,0), (-1,-1), 1, colors.white),  # All grid lines white\n",
    "    ('BOX', (0,0), (-1,-1), 1, colors.white),   # Outer box white\n",
    "    ('INNERGRID', (0,0), (-1,-1), 1, colors.white)  # Inner grid white\n",
    "])\n",
    "# Create the table\n",
    "# table = Table(data, style=style)\n",
    "# Apply style\n",
    "#table.setStyle(style)\n",
    "# story.append(table)\n",
    "#story.append(Spacer(1, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the PDF\n",
    "doc.build(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
